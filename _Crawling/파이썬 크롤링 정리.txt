관심있는 곳 데이터 수집해서. 
1. 크롤링과 스크레핑을 이용한 데이터 수집 그리고 형식을 본인이 만들어서 csv로 만들어서 저장하는 프로그램 짜기.
(갯수제한 없음)

2. 트위터나 Facebook이나 기상청이나 OPEN API 키를 발급받고, 발급된 키로 데이터 수집해서
CSV로 저장하기. (JSON이라 복잡하니까 그중에 몇가지만 저장하면 됨)

CSV저장 이미지 or 샘플 증명 이미지만

------------------------------------------------
CP949가 EUC_KR보다 확장 문자를 추가한 코드. CP949가 EUC_KR보다 많은 문자를 다룰 수 있음.
윈도우에선 EUC_KR = CP949 / 
유니코드문자 UTF-8

여러 페이지 연속크롤리잇 Session 객체를 사용하면 효과적. 

---------------------------------------------------------------------------------------
urllib 라이브러리 | url을 다루는 기능 제공 | 파이썬 기본 제공 | 안에 4개의 모듈이 있음.
- get,post같은 요청 처리시 간단, http 헤더 추가 또는 Basic 인증 등의 조금 더 나아간 처리시에는 굉장히 복잡. 하지만 requests 모듈사용시 문자코드 변환, 압축 등을 자동으로 처리해줌.

(1) urllib.request (웹 문서를 불러오기)
- r = req.urlopen("https://www.naver.com")
- r.getheaders() # 서버에 대한 정보를 리스트로 돌려줌. for문 써서 출력
- r.status() # 웹 페이지의 상태 확인하기
- r.read() # 웹 페이지의 데이터 읽어오기 (html로 읽어서 데이터 골라내기)
- urllib.urlretrieve(url, filesavename) # urlretrieve는 urlopen과 똑같지만. 바로 파일에 저장 하는 방식. urlopen은 임의 변수에 저장가능.

(2) urllib.parse 모듈 ( url에 한글 검색어를 입력할 수 있도록 도와주는 모델 - 한글로 검색하기위해서 한글을 %EB%AC .. 등으로 바꿔야 되는데 그 인코딩을 해주는 모듈) - 요청전용 URL생성
- urllib.parse.quote_plus 사용시 한글을 인코딩 해줌. (urllib.parse.quote도 같은 것)
- urllib.parse.urlencode(딕셔너리) -> 한글이나 특수문자 등 인코딩 해줌.

---------------------------------------------------------------------------------------
html 스크레이핑 라이브러리 
lxml과 beautifulsoup
xPath와 CSS선택자 
XPath는 //body/h1 방식.
CSS선택자는 > 방식.

body h1 : body요소의 후손중 h1요소  
body > h1 : body요소의 자식중 h1요소
body > * : body요소 내부의 모든 자식요소
#main : id속성이 main인요소
li.active : class속성으로 active를 포함하고 있는 li요소
input[type="text"] : type 속성이 'text'인input요소.
a[href^="http://"] : href속성이 "http://로 시작하는 a요소
---------------------------------------------------------------------------------------
beautiful soup
soup.h1 # 태그이름가진속성까지 출력
soup.h1.name # 태그이름추출
soup.h1.string # 태그 문자열만 추출
soup.u1.text # 요소 내부의 모든 문자열 추출
soup.h1['id'] # id 속성 출력
soup.h1['attr'] # attr 속성으로 모든 속성 딕셔너리로 출력.
soup.h1.parent # 부모 요소 추출
soup.li # 가장 앞에있는 li 속성 추출
soup.find('li') # 가장 앞에 있는 li속성 추출
soup.find_all('li') # li꺼 다찾기 
soup.find_all('li', class='featured') # feature 클래스거 다출력.
soup.select('li')  # 일치하는 요소 다 찾기
soup.select('li.featured') # 해당 속성꺼 찾기
soup.select('#main') # id가 main 인거 찾기.

--------------------------------------------------------------------------------------
트위터 API 사용하기.
- API종류 REST API와 STREAMING 두가지
- REST API (HTTP요청을 전송하면 HTTP 응답을 반환하는 RESTful 형태로 트윗 또는 사용자 정보 추출. 쓰기 권한 부여시 트윗올리기와 사용자 팔로우도 가능. 
- Streaming API  

-----------------------------------------------------------------------------------
Youtube API 사용하기.
- Youtube에서 제공하는 api를 이용해서 동영상을 검색할 수 있습니다. 검색만으로는 별로 쓸 데가 없겠지만, 이러한 검색 작업을 통해 영상의 제목, id 등을 파싱하고 결과적으로 Youtube동영상을 로컬로 다운받게 해주는 등 편리함을 제공

데이터는 xml, html, csv, json 방식 크롤링 스크레이핑 가능.

[1] xml은 rss라는 이름의 xml 형식으로 제공됨.
- xml파일 내려받아서 진행. rss라는 이름으 ㅣ요소를 루트로 하는 트리 구조 가지고 이씅ㅁ.
- 파이썬 자체에 ElementTree가 있음.  
- ex) tree = ElementTree.parse('rss.xml')

[2] csv(Comma Seperated Values)로 하나의 레코드를 한줄에 나타내고, 각 줄의 값을 쉼표로 구분.
(1) csv 형식 만들때 가장 쉬운 방법은 str.join() 메서드 사용하기.
ex) 
print(','.join(['1' , '상하이', '2415']))
print(','.join(['2' , '카라치', '2350']))
- 현재의 ','문자를 '\t'로 바꾸게 되면 파일이 TSV 파일이 됨.

csv 모듈 - csv.writer를 사용하면 한 줄 출력시 writerow() 메서드를 사용. 매개변수로 list또는 tuple과 같은 반복 가능한 객체를 전달.
ex) 매개변수로 리스트나 튜플 전달
writer.writerow(["a", "이름", "가격"])
writer.writerow(["b", "하이", "옵션"])
writer.writerow(["c", "쥬밤", "메롱"])
writer.writerows([
	[1, '상하이', 2415],
	[2, '도쿄', 3359],
	[3, '베이징', 5785],
	[4, '텐진', 1472] ])

---------------------------------------------------------------------------------------
JSON 형식
- 파이썬 자체에서 json모듈을 제공. 
- 배열 : 파이썬의 리스트와 유사 / 객체 : 파이썬의 딕셔너리와 유사
json.dumps() 함수를 사용시 list와 dict등의 객체를 json 형식 문자열로 바꿀 수 있음.
json.dumps() 함수의 두번째 매개변수에 파일 객체를 지정하면 해당파일 객체에 json을 출력.
- load 함수를 이용하여 읽어오기. 


brackets 정렬하기 -> 오른쪽마우스 클릭하고 beautify


-------------------------------------------------------------------------------------
 파이썬 리스트내 빈칸 지우기 
c_lst = [x for x in title if x]
title = c_lst